{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flaxmodels import ResNet18\n",
    "from typing import Callable\n",
    "import tensorflow\n",
    "import tqdm\n",
    "\n",
    "from optimizers import SGD, PAGE\n",
    "from dataloader import get_cifar10_dataloaders, tf_to_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(rng: jax.random.PRNGKey, init_batch: tuple[jnp.ndarray, jnp.ndarray]) -> tuple[nn.Module, dict]:\n",
    "    model = ResNet18(output=\"logits\",\n",
    "                     pretrained=False,\n",
    "                     num_classes=10)\n",
    "    variables = model.init(rng, init_batch[0])\n",
    "    return model, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss_fn(model: nn.Module, train: bool = True) -> Callable:\n",
    "    def loss_fn(variables: dict, batch: tuple[jnp.ndarray, jnp.ndarray]) -> jnp.ndarray:\n",
    "        images, labels = batch\n",
    "        outs  = model.apply(variables,\n",
    "                            images,\n",
    "                            mutable=[\"batch_stats\"] if train else None)\n",
    "        logits, new_batch_stats_dict = outs if train else (outs, {})\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "        return loss, new_batch_stats_dict.get(\"batch_stats\", {})\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_accuracy_fn(model: nn.Module) -> Callable:\n",
    "    def compute_accuracy(variables: dict, batch: tuple[jnp.ndarray, jnp.ndarray]) -> jnp.ndarray:\n",
    "        images, labels = batch\n",
    "        logits = model.apply(variables, images, train=False)\n",
    "        return jnp.mean(logits.argmax(axis=1) == labels)\n",
    "    return compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(variables: dict,\n",
    "          optimizer: SGD,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          num_epochs: int,\n",
    "\t\t  compute_accuracy) -> dict:\n",
    "    state = optimizer.init(variables)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_loader):\n",
    "            jax_batch = tf_to_jax(batch)\n",
    "            loss, state = optimizer.update(state, jax_batch)\n",
    "            print(f\"loss = {loss}\")\n",
    "        \n",
    "        val_acc, val_loss, val_count = 0, 0, 0\n",
    "        for batch in val_loader:\n",
    "            jax_batch = tf_to_jax(batch)\n",
    "            acc = compute_accuracy({\"params\": state.params, \"batch_stats\": state.batch_stats}, jax_batch)\n",
    "            val_acc += acc * batch[0].shape[0]\n",
    "            val_count += batch[0].shape[0]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: val_acc={val_acc / val_count}, val_loss={val_loss / val_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "BS, BS_HAT = 256, 16\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_cifar10_dataloaders(batch_size=BS)\n",
    "init_batch = tf_to_jax(next(iter(train_loader)))\n",
    "resnet18, variables = init_model(rng, init_batch)\n",
    "\n",
    "compute_accuracy = jax.jit(get_compute_accuracy_fn(resnet18))\n",
    "\n",
    "train_loss_fn = get_cross_entropy_loss_fn(resnet18, train=True)\n",
    "eval_loss_fn = get_cross_entropy_loss_fn(resnet18, train=False)\n",
    "# optimizer = SGD(train_loss_fn,\n",
    "#                 lr=1e-4,\n",
    "#                 need_jit=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = PAGE(train_loss_fn,\n",
    "                 eval_loss_fn,\n",
    "                 p=BS_HAT / (BS + BS_HAT),\n",
    "                 lr=1e-4,\n",
    "                 bs=BS,\n",
    "                 bs_hat=BS_HAT,\n",
    "                 need_jit=True)\n",
    "\n",
    "train(variables=variables,\n",
    "      optimizer=optimizer,\n",
    "      train_loader=train_loader,\n",
    "      val_loader=val_loader,\n",
    "      num_epochs=10,\n",
    "      compute_accuracy=compute_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
