{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from optimizers import SGD\n",
    "from test_opts import MLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDIM = 128\n",
    "BS = 256\n",
    "\n",
    "true_A = jax.random.uniform(KEY, (XDIM, 1))\n",
    "\n",
    "def generate_batch(batch_size=BS, key: jax.random.PRNGKey=KEY) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    x = jax.random.uniform(key, (batch_size, XDIM), minval=0, maxval=3)\n",
    "    y = jnp.dot(x, true_A) + jax.random.normal(key, (batch_size, 1))*0.01\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "x0, _ = generate_batch(BS)\n",
    "variables = model.init(KEY, x0)\n",
    "\n",
    "def loss_fn(variables, batch: tuple[jnp.ndarray, jnp.ndarray]) -> jnp.ndarray:\n",
    "    x, y = batch\n",
    "    pred = model.apply(variables, x)\n",
    "    return jnp.mean((pred - y) ** 2), {}\n",
    "\n",
    "opt = SGD(lr=1e-5, loss_fn=loss_fn)\n",
    "state =opt.init(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 9731.4961\n",
      "Step 100 | Loss: 17.4083\n",
      "Step 200 | Loss: 13.2847\n",
      "Step 300 | Loss: 14.7276\n",
      "Step 400 | Loss: 12.5477\n",
      "Step 499 | Loss: 10.5139\n",
      "Step 500 | Loss: 12.9076\n",
      "Step 600 | Loss: 11.9682\n",
      "Step 700 | Loss: 10.6699\n",
      "Step 800 | Loss: 11.7378\n",
      "Step 900 | Loss: 10.5600\n",
      "Step 1000 | Loss: 8.6733\n",
      "Step 1100 | Loss: 7.6996\n",
      "Step 1200 | Loss: 6.5770\n",
      "Step 1300 | Loss: 7.3297\n",
      "Step 1400 | Loss: 5.4491\n",
      "Step 1500 | Loss: 6.5307\n",
      "Step 1600 | Loss: 6.6463\n",
      "Step 1700 | Loss: 5.5747\n",
      "Step 1800 | Loss: 4.6180\n",
      "Step 1900 | Loss: 5.0443\n",
      "Step 2000 | Loss: 4.4707\n",
      "Step 2100 | Loss: 4.5855\n",
      "Step 2200 | Loss: 4.0886\n",
      "Step 2300 | Loss: 3.4814\n",
      "Step 2400 | Loss: 4.2954\n",
      "Step 2500 | Loss: 3.5235\n",
      "Step 2600 | Loss: 3.1237\n",
      "Step 2700 | Loss: 2.9037\n",
      "Step 2800 | Loss: 2.7097\n",
      "Step 2900 | Loss: 2.4445\n",
      "Step 3000 | Loss: 2.6034\n",
      "Step 3100 | Loss: 2.5764\n",
      "Step 3200 | Loss: 2.1370\n",
      "Step 3300 | Loss: 2.4212\n",
      "Step 3400 | Loss: 1.9206\n",
      "Step 3500 | Loss: 1.7639\n",
      "Step 3600 | Loss: 2.2090\n",
      "Step 3700 | Loss: 1.2971\n",
      "Step 3800 | Loss: 1.4001\n",
      "Step 3900 | Loss: 1.5346\n",
      "Step 4000 | Loss: 1.4315\n",
      "Step 4100 | Loss: 1.3698\n",
      "Step 4200 | Loss: 1.4884\n",
      "Step 4300 | Loss: 1.3506\n",
      "Step 4400 | Loss: 1.1406\n",
      "Step 4500 | Loss: 1.1467\n",
      "Step 4600 | Loss: 1.0682\n",
      "Step 4700 | Loss: 1.0269\n",
      "Step 4800 | Loss: 1.0726\n",
      "Step 4900 | Loss: 0.8208\n",
      "Step 5000 | Loss: 0.8401\n",
      "Step 5100 | Loss: 0.8767\n",
      "Step 5200 | Loss: 0.7619\n",
      "Step 5300 | Loss: 0.6834\n",
      "Step 5400 | Loss: 0.7693\n",
      "Step 5500 | Loss: 0.7094\n",
      "Step 5600 | Loss: 0.6563\n",
      "Step 5700 | Loss: 0.5067\n",
      "Step 5800 | Loss: 0.5902\n",
      "Step 5900 | Loss: 0.5524\n",
      "Step 6000 | Loss: 0.5904\n",
      "Step 6100 | Loss: 0.4836\n",
      "Step 6200 | Loss: 0.5052\n",
      "Step 6300 | Loss: 0.4373\n",
      "Step 6400 | Loss: 0.4840\n",
      "Step 6500 | Loss: 0.5081\n",
      "Step 6600 | Loss: 0.4144\n",
      "Step 6700 | Loss: 0.4413\n",
      "Step 6800 | Loss: 0.3599\n",
      "Step 6900 | Loss: 0.4309\n",
      "Step 7000 | Loss: 0.3837\n",
      "Step 7100 | Loss: 0.3816\n",
      "Step 7200 | Loss: 0.3416\n",
      "Step 7300 | Loss: 0.3180\n",
      "Step 7400 | Loss: 0.3134\n",
      "Step 7500 | Loss: 0.2398\n",
      "Step 7600 | Loss: 0.2920\n",
      "Step 7700 | Loss: 0.3287\n",
      "Step 7800 | Loss: 0.3106\n",
      "Step 7900 | Loss: 0.3255\n",
      "Step 8000 | Loss: 0.2672\n",
      "Step 8100 | Loss: 0.2906\n",
      "Step 8200 | Loss: 0.3017\n",
      "Step 8300 | Loss: 0.2705\n",
      "Step 8400 | Loss: 0.2815\n",
      "Step 8500 | Loss: 0.3090\n",
      "Step 8600 | Loss: 0.2748\n",
      "Step 8700 | Loss: 0.2417\n",
      "Step 8800 | Loss: 0.2462\n",
      "Step 8900 | Loss: 0.2587\n",
      "Step 9000 | Loss: 0.2419\n",
      "Step 9100 | Loss: 0.2487\n",
      "Step 9200 | Loss: 0.2255\n",
      "Step 9300 | Loss: 0.2479\n",
      "Step 9400 | Loss: 0.2447\n",
      "Step 9500 | Loss: 0.1876\n",
      "Step 9600 | Loss: 0.2016\n",
      "Step 9700 | Loss: 0.1920\n",
      "Step 9800 | Loss: 0.2296\n",
      "Step 9900 | Loss: 0.2203\n",
      "Step 10000 | Loss: 0.2070\n",
      "Step 10100 | Loss: 0.2353\n",
      "Step 10200 | Loss: 0.2010\n",
      "Step 10300 | Loss: 0.1704\n",
      "Step 10400 | Loss: 0.1973\n",
      "Step 10500 | Loss: 0.1921\n",
      "Step 10600 | Loss: 0.2085\n",
      "Step 10700 | Loss: 0.1805\n",
      "Step 10800 | Loss: 0.1765\n",
      "Step 10900 | Loss: 0.2049\n",
      "Step 11000 | Loss: 0.2101\n",
      "Step 11100 | Loss: 0.1621\n",
      "Step 11200 | Loss: 0.1733\n",
      "Step 11300 | Loss: 0.1667\n",
      "Step 11400 | Loss: 0.1770\n",
      "Step 11500 | Loss: 0.1760\n",
      "Step 11600 | Loss: 0.1521\n",
      "Step 11700 | Loss: 0.1862\n",
      "Step 11800 | Loss: 0.1834\n",
      "Step 11900 | Loss: 0.1803\n",
      "Step 12000 | Loss: 0.2007\n",
      "Step 12100 | Loss: 0.1991\n",
      "Step 12200 | Loss: 0.2009\n",
      "Step 12300 | Loss: 0.1822\n",
      "Step 12400 | Loss: 0.1677\n",
      "Step 12500 | Loss: 0.1809\n",
      "Step 12600 | Loss: 0.1766\n",
      "Step 12700 | Loss: 0.1703\n",
      "Step 12800 | Loss: 0.1638\n",
      "Step 12900 | Loss: 0.2001\n",
      "Step 13000 | Loss: 0.1887\n",
      "Step 13100 | Loss: 0.1750\n",
      "Step 13200 | Loss: 0.1886\n",
      "Step 13300 | Loss: 0.1708\n",
      "Step 13400 | Loss: 0.1772\n",
      "Step 13500 | Loss: 0.1676\n",
      "Step 13600 | Loss: 0.1703\n",
      "Step 13700 | Loss: 0.1502\n",
      "Step 13800 | Loss: 0.1588\n",
      "Step 13900 | Loss: 0.1786\n",
      "Step 14000 | Loss: 0.1567\n",
      "Step 14100 | Loss: 0.1487\n",
      "Step 14200 | Loss: 0.1755\n",
      "Step 14300 | Loss: 0.1725\n",
      "Step 14400 | Loss: 0.1491\n",
      "Step 14500 | Loss: 0.1598\n",
      "Step 14600 | Loss: 0.1512\n",
      "Step 14700 | Loss: 0.1788\n",
      "Step 14800 | Loss: 0.1578\n",
      "Step 14900 | Loss: 0.1661\n",
      "Step 15000 | Loss: 0.1226\n",
      "Step 15100 | Loss: 0.1750\n",
      "Step 15200 | Loss: 0.1601\n",
      "Step 15300 | Loss: 0.1502\n",
      "Step 15400 | Loss: 0.1633\n",
      "Step 15500 | Loss: 0.1472\n",
      "Step 15600 | Loss: 0.1484\n",
      "Step 15700 | Loss: 0.1774\n",
      "Step 15800 | Loss: 0.1463\n",
      "Step 15900 | Loss: 0.1871\n",
      "Step 16000 | Loss: 0.1404\n",
      "Step 16100 | Loss: 0.1418\n",
      "Step 16200 | Loss: 0.1485\n",
      "Step 16300 | Loss: 0.1458\n",
      "Step 16400 | Loss: 0.1280\n",
      "Step 16500 | Loss: 0.1725\n",
      "Step 16600 | Loss: 0.1725\n",
      "Step 16700 | Loss: 0.1299\n",
      "Step 16800 | Loss: 0.1484\n",
      "Step 16900 | Loss: 0.1438\n",
      "Step 17000 | Loss: 0.1602\n",
      "Step 17100 | Loss: 0.1634\n",
      "Step 17200 | Loss: 0.1371\n",
      "Step 17300 | Loss: 0.1444\n",
      "Step 17400 | Loss: 0.1328\n",
      "Step 17500 | Loss: 0.1650\n",
      "Step 17600 | Loss: 0.1485\n",
      "Step 17700 | Loss: 0.1356\n",
      "Step 17800 | Loss: 0.1307\n",
      "Step 17900 | Loss: 0.1422\n",
      "Step 18000 | Loss: 0.1461\n",
      "Step 18100 | Loss: 0.1481\n",
      "Step 18200 | Loss: 0.1638\n",
      "Step 18300 | Loss: 0.1561\n",
      "Step 18400 | Loss: 0.1324\n",
      "Step 18500 | Loss: 0.1204\n",
      "Step 18600 | Loss: 0.1602\n",
      "Step 18700 | Loss: 0.1232\n",
      "Step 18800 | Loss: 0.1382\n",
      "Step 18900 | Loss: 0.1308\n",
      "Step 19000 | Loss: 0.1481\n",
      "Step 19100 | Loss: 0.1455\n",
      "Step 19200 | Loss: 0.1293\n",
      "Step 19300 | Loss: 0.1459\n",
      "Step 19400 | Loss: 0.1596\n",
      "Step 19500 | Loss: 0.1352\n",
      "Step 19600 | Loss: 0.1240\n",
      "Step 19700 | Loss: 0.1346\n",
      "Step 19800 | Loss: 0.1327\n",
      "Step 19900 | Loss: 0.1525\n",
      "CPU times: user 4min 40s, sys: 1min 26s, total: 6min 6s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "losses = []\n",
    "\n",
    "for step in range(20_000):\n",
    "\ttrain_key, KEY = jax.random.split(KEY)\n",
    "\tbatch = generate_batch(BS, train_key)\n",
    "\tloss, state = opt.update(state, batch)\n",
    "\tif step % 100 == 0 or step == 499:\n",
    "\t\tlosses.append(loss)\n",
    "\t\tprint(f\"Step {step} | Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
